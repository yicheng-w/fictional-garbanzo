{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_loc = \"../aclImdb\"\n",
    "\n",
    "def build_dataset(root):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(os.path.join(root, 'pos')):\n",
    "        data.append(open(os.path.join(root, 'pos', filename)).read())\n",
    "        labels.append(1)\n",
    "    for filename in os.listdir(os.path.join(root, 'neg')):\n",
    "        data.append(open(os.path.join(root, 'neg', filename)).read())\n",
    "        labels.append(0)\n",
    "    return data, labels\n",
    "\n",
    "class IMDBDataset(object):\n",
    "    def __init__(self, root):\n",
    "        self.train_data, self.train_labels = build_dataset(os.path.join(root, 'train'))\n",
    "        self.dev_data, self.dev_labels = build_dataset(os.path.join(root, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbDataset = IMDBDataset(data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building vocab! Vocab size: 12314\n",
      "Done building train dev!\n",
      "Training...\n",
      "Iteration 1, loss = 0.34067612\n",
      "Iteration 2, loss = 0.15657234\n",
      "Iteration 3, loss = 0.06402856\n",
      "Iteration 4, loss = 0.01649610\n",
      "Iteration 5, loss = 0.00646667\n",
      "Iteration 6, loss = 0.00777718\n",
      "Iteration 7, loss = 0.01170576\n",
      "Iteration 8, loss = 0.01035176\n",
      "Iteration 9, loss = 0.01007133\n",
      "Iteration 10, loss = 0.00332316\n",
      "Iteration 11, loss = 0.00200266\n",
      "Iteration 12, loss = 0.00059611\n",
      "Iteration 13, loss = 0.00044441\n",
      "Iteration 14, loss = 0.00042735\n",
      "Iteration 15, loss = 0.00042113\n",
      "Iteration 16, loss = 0.00041667\n",
      "Iteration 17, loss = 0.00041297\n",
      "Iteration 18, loss = 0.00040972\n",
      "Iteration 19, loss = 0.00040679\n",
      "Iteration 20, loss = 0.00040407\n",
      "Iteration 21, loss = 0.00040149\n",
      "Iteration 22, loss = 0.00039900\n",
      "Iteration 23, loss = 0.00039657\n",
      "Iteration 24, loss = 0.00039418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training done!\n",
      "Testing on dev...\n",
      "Done! Scores below:\n",
      "0.8732\n",
      "0.85664\n"
     ]
    }
   ],
   "source": [
    "from BagOfWordsNN import BoWMLP\n",
    "clf = BoWMLP(imdbDataset.train_data, imdbDataset.train_labels, rare_word_threshold=15)\n",
    "clf.fit()\n",
    "clf.test(imdbDataset.dev_data, imdbDataset.dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BoWMLP-IMDB.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(clf, \"BoWMLP-IMDB.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BagOfWordsLogReg import BoWLR\n",
    "clf2 = BoWLR(imdbDataset.train_data, imdbDataset.train_labels, rare_word_threshold=15)\n",
    "clf2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86056\n"
     ]
    }
   ],
   "source": [
    "clf2.test(imdbDataset.dev_data, imdbDataset.dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BoWLogReg-IMDB.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf2, \"BoWLogReg-IMDB.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying on test points\n",
    "\n",
    "import joblib\n",
    "from BagOfWordsLogReg import BoWLR\n",
    "from BagOfWordsNN import BoWMLP\n",
    "\n",
    "clf_lr = joblib.load(\"BoWLogReg-IMDB.joblib\")\n",
    "clf_nn = joblib.load(\"BoWMLP-IMDB.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2401\n"
     ]
    }
   ],
   "source": [
    "test_point_file_loc = \"/home/yicheng-wang/CS-Stuff/machine_learning/Sentence-VAE/test_points.txt\"\n",
    "\n",
    "with open(test_point_file_loc, 'r') as in_file:\n",
    "    test_points = in_file.read().split('\\n')[:-1]\n",
    "    \n",
    "print(len(test_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2401,)\n"
     ]
    }
   ],
   "source": [
    "preds_lr = clf_lr.classify(test_points)\n",
    "print(preds_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2401,)\n"
     ]
    }
   ],
   "source": [
    "preds_nn = clf_nn.classify(test_points)\n",
    "print(preds_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_decision_boundary, write_out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_boundaries = find_decision_boundary(preds_lr, 4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_boundaries = find_decision_boundary(preds_nn, 4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred_file_loc = \"/home/yicheng-wang/CS-Stuff/machine_learning/fictional-garbanzo/imdb_test_points.h5f\"\n",
    "\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(lstm_pred_file_loc, 'r')\n",
    "rnn_preds = f['predictions'][:]\n",
    "f.close()\n",
    "lstm_boundaries = find_decision_boundary(rnn_preds, 4, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "511\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "print(len(lr_boundaries))\n",
    "print(len(nn_boundaries))\n",
    "print(len(lstm_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2l = {0: 'negative', 1: 'positive'}\n",
    "write_out_csv(test_points, preds_lr, lr_boundaries, p2l, 'imdb_lr_boundary.csv')\n",
    "write_out_csv(test_points, preds_nn, nn_boundaries, p2l, 'imdb_mlp_boundary.csv')\n",
    "write_out_csv(test_points, rnn_preds, lstm_boundaries, p2l, 'imdb_rnn_boundary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
